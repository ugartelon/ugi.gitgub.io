<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>RSelenium &amp; Web Scraper Chrome to gather data | A minimal Hugo website</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/about/">About</a></li>
      
      <li><a href="/categories/">Categories</a></li>
      
      <li><a href="/cv/">CV</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">RSelenium &amp; Web Scraper Chrome to gather data</span></h1>
<h2 class="author">Pablo Ugarte</h2>
<h2 class="date">2017/09/21</h2>
</div>

<main>
<div id="intro" class="section level1">
<h1>Intro</h1>
<blockquote>
<p>In this post, we scrape data and data wrangle to get a picture of type oj jobs offered in the expansion website.</p>
</blockquote>
</div>
<div id="libraries" class="section level1">
<h1>Libraries</h1>
<p>Overall, we are going to use the following R packages:</p>
<pre class="r"><code>library(XML) # For web scraping
library(rvest) # For web scraping
library(tidyverse) # Data wrangling packages gather, spread...
library(lubridate) # To work with dates
library(hrbrthemes) # Aesthetic defaults for ggplot2 charts
library(stringr) # For manipulating strings</code></pre>
<p>One way would be to make vector of all the urls you are interested in and then use sapply.</p>
<p>Results should be a list of 6 data.frame objects (pages - 0,1,2,3,5,6); each should be named with the url (i.e., page) they represent. That is, results[1] corresponds to the first page, and results[5] corresponds to last page <a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>.</p>
<p>To convert a list of data frames into one data frame we use the dplyr::rbind_all(). This will do the trick ;) <a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>.</p>
<pre class="r"><code>pages &lt;- 0:5
urls &lt;- paste0(&quot;http://www.expansionyempleo.com/buscar-trabajo-empleo/cid/3BC18A08D9BB661C9B65C93203B654EF/canal/0/pagenumber/&quot;,pages)

get_table &lt;- function(url) {
  url %&gt;%
    read_html() %&gt;%
    html_nodes(xpath = &#39;//*[@id=&quot;mytable&quot;]&#39;) %&gt;% 
    html_table()
}

results &lt;- sapply(urls, get_table)
results &lt;- rbind_all(results)
View(results)</code></pre>
<p>However, I am getting an error when I increase the number of pages to more than 5. So I decided to use the Web Scraper Chrome extension <a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> and download the file to csv.</p>
<pre class="r"><code>expansion2 &lt;- read_csv(&quot;~/Documents/R Working Directory/Blog/First/content/post/expansion2.csv&quot;)</code></pre>
</div>
<div id="cleansing" class="section level1">
<h1>Cleansing</h1>
<p>We now we can start our analysis.</p>
<p>The dataframe has 5 columns:</p>
<ul>
<li>Fecha (date - format %d/%m)</li>
<li>Cargo (job)</li>
<li>Empresa (company/headhunter)</li>
<li>Vacantes (vacancies)</li>
<li>Zona (city)</li>
</ul>
<p>Expansion is the main financial newspaper in Spain and hence quality of the jobs posted are meant to be above average (in terms of salary and job responsabilities).</p>
<p>However, by having a quick read looks like becas/practicas (internships) seem to be fairly substantial.</p>
<p>Let’s go into more detail and get a sense of the proportion of “bad quality jobs” out of the total.</p>
<p>We first convert selected columns of the dataframe to lower case. We will aso clean text from punctutation using R Base gsub.</p>
<pre class="r"><code>expansion2 &lt;- expansion2 %&gt;% mutate(Cargo = tolower(Cargo)) %&gt;% mutate(Empresa = tolower(Empresa)) %&gt;% mutate(Zona = tolower(Zona))
expansion2$Cargo &lt;- gsub(pattern=&quot;[[:punct:]]&quot;, expansion2$Cargo, replacement=&quot; &quot;)</code></pre>
<p>We can find out the jobs (Cargo) that contain ‘prácticas’ or ‘beca’ (or related like becario) by using ‘str_detect’ function from ‘stringr’ package.</p>
<p>We use it inside ‘mutate’ command so that it will return TRUE or FALSE. And then calculate the percentage we seek.</p>
<pre class="r"><code># Create a vector of a list of names I want to extract into a new column in the dataframe
extract &lt;- c(&quot;beca&quot;,&quot;becario&quot;, &quot;prácticas&quot;, &quot;práctica&quot;, &quot;practicas&quot;)
#  str_detect only accepts a length-1 pattern so we use paste(..., collapse = &#39;|&#39;)
expansion2 &lt;- expansion2 %&gt;% mutate(subpar = str_detect(expansion2$Cargo, paste(extract, collapse = &#39;|&#39;)))
# Lets&#39;s change the following subpar’ names using ‘recode’ function from dplyr package
expansion2 &lt;- expansion2 %&gt;% 
  mutate(subpar=replace(subpar, subpar==FALSE, &quot;Good quality&quot;)) %&gt;%
  mutate(subpar=replace(subpar, subpar==TRUE, &quot;Bad quality&quot;)) </code></pre>
<p>Now, let’s say we want to understand what are the percentage of low quality vacancies/jobs out of the total.</p>
<pre class="r"><code>qualityjobs &lt;- expansion2 %&gt;% 
  group_by(subpar) %&gt;%
  summarise (n = sum(Vacantes)) %&gt;%
  mutate(freq = n / sum(n))
View(expansion2)
View(qualityjobs)</code></pre>
<p>Let’s plot the above.</p>
<pre class="r"><code>ggplot(data=qualityjobs, aes(subpar, freq)) +
geom_col() +
scale_y_percent() +
labs(x=&quot;Type of jobs&quot;, y=&quot;Share (%)&quot;,
       title=&quot;Proportion of jobs that are &#39;good&#39; and &#39;bad&#39; quality&quot;,
       subtitle=&quot;Data extracted with Web Scraper Chrome extension&quot;,
       caption=&quot;Source: Expansion empleo&quot;) + 
theme_ipsum(grid=&quot;Y&quot;)</code></pre>
<p><img src="/post/2017-09-21-using-rselenium-to-gather-data_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Now, let’s see which city offers more bad quality jobs. We will select the top 5 cities for each type of quality job based on frequency (in % terms of total by that city).</p>
<pre class="r"><code>qualityjobszona &lt;- expansion2 %&gt;% 
  group_by(Zona, subpar) %&gt;%
  summarise (n = sum(Vacantes)) %&gt;%
  mutate(freq = n / sum(n)) %&gt;%
  filter(n &gt; 10) %&gt;%
  filter(subpar == &quot;Bad quality&quot;) %&gt;%
  arrange(desc(freq)) %&gt;%
  top_n(5, freq)
View(qualityjobszona)</code></pre>
<p>Let’s plot the above.</p>
<pre class="r"><code>ggplot(data=qualityjobszona, aes(reorder(Zona, -freq), freq)) +
geom_col() +
scale_y_percent() +
labs(x=&quot;Type of jobs&quot;, y=&quot;Share (%)&quot;,
       title=&quot;Cities with most &#39;bad&#39; quality jobs posted&quot;,
       subtitle=&quot;Data extracted with Web Scraper Chrome extension&quot;,
       caption=&quot;Source: Expansion empleo&quot;) + 
theme_ipsum(grid=&quot;Y&quot;) </code></pre>
<p><img src="/post/2017-09-21-using-rselenium-to-gather-data_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>In this article, we have briefly explored the web scraping of public data shown some of the R’s power for data analysis.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Link1 (<a href="https://stackoverflow.com/questions/40140133/scraping-tables-on-multiple-web-pages-with-rvest-in-r" class="uri">https://stackoverflow.com/questions/40140133/scraping-tables-on-multiple-web-pages-with-rvest-in-r</a>)<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Link2 (<a href="https://stackoverflow.com/questions/2851327/convert-a-list-of-data-frames-into-one-data-frame" class="uri">https://stackoverflow.com/questions/2851327/convert-a-list-of-data-frames-into-one-data-frame</a>)<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Lik3 (<a href="http://webscraper.io/" class="uri">http://webscraper.io/</a>)<a href="#fnref3">↩</a></p></li>
</ol>
</div>

</main>

  <footer>
  <script src="//yihui.name/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.name/js/center-img.js"></script>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-77339605-2', 'auto');
ga('send', 'pageview');
</script>


  
  <hr/>
  &copy; Pablo Ugarte 2017 | <a href="https://www.linkedin.com/in/pablo-ugarte-19b11232/">Linkedin</a>
  
  </footer>
  </body>
</html>

