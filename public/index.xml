<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home on A minimal Hugo website</title>
    <link>https://www.storieswithdata.com/</link>
    <description>Recent content in Home on A minimal Hugo website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 17 Sep 2017 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://www.storieswithdata.com/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>cv</title>
      <link>https://www.storieswithdata.com/cv/</link>
      <pubDate>Sun, 17 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.storieswithdata.com/cv/</guid>
      <description> Pablo Ugarte ✉ : ugartelon@gmail.com
✆ : (+34) 656 45 17 17
EXPERIENCIA LABORAL MASTER IN DATA ANALYTICS ISDI, MADRID (Octubre 2016 – Agosto 2017)  Resolución de problemas y situaciones de negocio a partir del aná lisis de la información Uso de herramientas de analıtica en el ánalisis de datos (R, Google Analytics y Tableau)  BANK OF ENGLAND - THE FINANCIAL SERVICES AUTHORITY, LONDRES (Marzo 2009 – Septiembre 2016)  Resolución de problemas y situaciones de negocio a partir del análisis de la información Análisis de estados financieros y planes estratégicos de las mayores entidades británicas Análisis de sensibilidad del sistema bancario británico mediante pruebas de resistencia – stress testing Preparación de informes y estadı́sticas para el Financial Policy Committee (FPC) Representante de la divisió n Financial Stability, Strategy &amp;amp; Risk (FSSR) en asuntos relacionados con FinTech Análisis financiero de las solicitudes para constituirse como banco en UK  CITIGROUP, LONDRES (Abril 2008 – Febrero 2009)  Elaboración de órdenes de inversión y desinversión solicitadas por los clientes Control de las carteras de los clientes para que no se desvı́en de los objetivos originales Gestión de cash management y estrategias de cobertura Preparación de los informes de pérdidas y ganancias  STANDARD &amp;amp; POOR’S, LONDRES (Abril 2007 – Febrero 2008)  Análisis y elaboración de rating de entidades financieras de UK Elaboración de informes sectoriales y de coyuntura económica con el fin de analizar la información financiera y estadı́stica de la entidad Seguimiento perió dico de la calidad de la deuda de las entidades analizadas  FITCH RATINGS, LONDRES (Noviembre 2005 – Febrero 2007)  Análisis y elaboración de rating de entidades financieras de España, Portugal y Andorra Elaboración de informes sectoriales y de coyuntura económica con el fin de analizar la información financiera y estadı́stica de la entidad Seguimiento perió dico de la calidad de la deuda de las entidades analizadas  EDUCACIÓN Y FORMACIÓN  Master in Data Analytics ISDI (2016 - 2017) CertificaciónGoogleAnalytics (GAIQ) (2017) Investment Management Certificate (IMC) (2008) Licenciado en Ciencias Empresariales ICADE (E2) (2000 - 2005) Colegio Nuestra Señora del Recuerdo (Jesuitas) (1987 - 2000)  APTITUDES Y AFICIONES  IT: R, Bloomberg, Dealogic, Microsoft Office, Google Analytics y Tableau Información adicional: Deporte, lectura, cine, documentales y viajar  </description>
    </item>
    
    <item>
      <title>Unemployment in Spain</title>
      <link>https://www.storieswithdata.com/post/2017/09/15/unemployment-in-spain/</link>
      <pubDate>Fri, 15 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.storieswithdata.com/post/2017/09/15/unemployment-in-spain/</guid>
      <description>Intro  In this post, we scrape data and data wrangle to get an overall picute of the job market.
 The data comes from the Instituto Nacional de Estadistica (INE) website 1.
 Libraries Overall, we are going to use the following R packages:
library(XML) # For web scraping library(rvest) # For web scraping library(dplyr) # For dataset manipulation library(hrbrthemes) # Aesthetic defaults for ggplot2 charts library(ggthemes) # Themes ggplot2 library(ggplot2) # Charts library(stringr) # Manipulating strings library(tidyverse) # Data wrangling packages gather, spread.</description>
    </item>
    
    <item>
      <title>Electric cars and metal prices</title>
      <link>https://www.storieswithdata.com/post/2017/09/14/electric-cars-and-metal-prices/</link>
      <pubDate>Thu, 14 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.storieswithdata.com/post/2017/09/14/electric-cars-and-metal-prices/</guid>
      <description>In this post we will analyze the evolution of metal prices on the back of the Electric car revolution
 Intro The electric vehicles industry is creating winners and losers within the world’s biggest metals markets.
In this regard, Cobalt is essential for lithium-ion batteries powering anything from Tesla Inc.’s cars to Apple Inc.’s iPhones and iPads 1.
 Libraries We first install and load the libraries.
library(ggplot2) # For amazing charts library(dplyr) # Data wrangling library(hrbrthemes) # Aesthetic defaults for ggplot2 charts library(Quandl) # Get financial data library(magrittr) # Pipes %&amp;gt;% library(tidyverse) # Data wrangling packages  Importing data We start by getting the prices for Cobalt, Nickel, Aluminium and Copper 2.</description>
    </item>
    
    <item>
      <title>BiciMad and Heatmaps</title>
      <link>https://www.storieswithdata.com/post/2017/09/13/bicimad-and-heatmaps/</link>
      <pubDate>Wed, 13 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.storieswithdata.com/post/2017/09/13/bicimad-and-heatmaps/</guid>
      <description>In this post we will analyze Madrid’s electric bike hire service, BiciMAD (in London Santander Cycles, in Paris Vélib, etc…). We will create calendar heatmaps and a growth chart using the publicly available data.
 Libraries We first install and load the libraries.
library(ggplot2) # For amazing charts library(tidyverse) # Data wrangling library(hrbrthemes) # Aesthetic defaults for ggplot2 charts library(readr) # Reading data from csv library(formattable) # Friendy data frame can be rendered as HTML table library(lubridate) # Makes it easier to work with dates and times  Importing data The data can be obtained from datosabiertos - the free and open data-sharing portal where anyone can access data relating to the Madrid 1.</description>
    </item>
    
    <item>
      <title>Joyplots with ggjoy and hrbrthmes</title>
      <link>https://www.storieswithdata.com/post/2017/09/12/joyplots-with-ggjoy-and-hrbrthmes/</link>
      <pubDate>Tue, 12 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.storieswithdata.com/post/2017/09/12/joyplots-with-ggjoy-and-hrbrthmes/</guid>
      <description>Haven’t you run into articles with some data that makes you think, how can visualize the data in a different and possibly better way?
 We will be use Joyplots for this post.
Joyplots are partially overlapping line plots that create the impression of a mountain range. They can be quite useful for visualizing changes in distributions over time or space 1.
The data that we will be looking at is 2016 temperatures in Lincoln, NE 23.</description>
    </item>
    
    <item>
      <title>Tennis winners - scraping and plotting</title>
      <link>https://www.storieswithdata.com/post/2017/09/11/tennis-winners-scrapig-and-plotting/</link>
      <pubDate>Mon, 11 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.storieswithdata.com/post/2017/09/11/tennis-winners-scrapig-and-plotting/</guid>
      <description>Intro  In this tutorial, we will be plotting tennis players Grand Slam Tournament wins.
 The data comes from the Tennis Grand Slam Tournaments results from the ESP website 1.
We will scrape the data and continue with the visualization thereafter.
 Libraries Overall, we are going to use the following R packages:
library(XML) # for web scraping library(rvest) # for web scraping library(dplyr) # for dataset manipulation library(knitr) # for nice dataset printing library(hrbrthemes) # Aesthetic defaults for ggplot2 charts library(ggthemes) # Themes ggplot2 library(ggplot2) # Charts library(stringr) # Manipulating strings  Scraping We will be scraping HTML Table with the XML package.</description>
    </item>
    
    <item>
      <title>JSON and Maps</title>
      <link>https://www.storieswithdata.com/post/2017/09/10/json-and-maps/</link>
      <pubDate>Sun, 10 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.storieswithdata.com/post/2017/09/10/json-and-maps/</guid>
      <description>Today we will extract information from a JSON document using the R package jsonlite and we will then plot lon/lat in a map using the R package leaflet.
The data that we will be looking at is crime in LA.
But first, we need to install and load libraries for this excercise.
library(jsonlite) library(tidyverse) library(stringr) library(tibble) library(leaflet) library(ggmap) library(readr) Now we can start grabbing the data from the json document which can be found here 1.</description>
    </item>
    
    <item>
      <title>A Quick Note on Building Websites</title>
      <link>https://www.storieswithdata.com/post/2017/09/09/a-quick-note-on-building-websites/</link>
      <pubDate>Sat, 09 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.storieswithdata.com/post/2017/09/09/a-quick-note-on-building-websites/</guid>
      <description>Note: Thanks to those who share their ideas and help others make things happen seamlessly.
 Finally! I now have a blog. All thanks to the R package Blogdown. With Blogdown you can creat a personal website.
 After reading many articles, watching videos and multiple failed attempts, I was able to get my picked Hugo theme up and running with R Studio and Blogdown.
There are many articles on the topic but the place I strongly recommend you visit is a youtube video by John Muschelli:</description>
    </item>
    
    <item>
      <title>Pablo Ugarte</title>
      <link>https://www.storieswithdata.com/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.storieswithdata.com/about/</guid>
      <description>After a hard time getting around creating the blog, here it is!
The goal of this blog is to collect, inspect, cleanse, transform, and visualize data to discover useful information and suggest conclusions. Data that we can find anywhere these days thanks to the web. The idea is ultimatley to have fun working with such data. We will be using R to perform all the analysis - and in particular dplyr.</description>
    </item>
    
  </channel>
</rss>