<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home on A minimal Hugo website</title>
    <link>https://www.storieswithdata.com/</link>
    <description>Recent content in Home on A minimal Hugo website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 15 Sep 2017 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://www.storieswithdata.com/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Unemployment in Spain</title>
      <link>https://www.storieswithdata.com/post/2017/09/15/unemployment-in-spain/</link>
      <pubDate>Fri, 15 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.storieswithdata.com/post/2017/09/15/unemployment-in-spain/</guid>
      <description>Intro  In this post, we scrape data and data wrangle to get an overall picute of the job market.
 The data comes from the Instituto Nacional de Estadistica (INE) website 1.
 Libraries Overall, we are going to use the following R packages:
library(XML) # For web scraping library(rvest) # For web scraping library(dplyr) # For dataset manipulation library(hrbrthemes) # Aesthetic defaults for ggplot2 charts library(ggthemes) # Themes ggplot2 library(ggplot2) # Charts library(stringr) # Manipulating strings library(tidyverse) # Data wrangling packages gather, spread.</description>
    </item>
    
    <item>
      <title>Unemployment in Spain</title>
      <link>https://www.storieswithdata.com/post/2017/09/15/unemployment-in-spain/</link>
      <pubDate>Fri, 15 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.storieswithdata.com/post/2017/09/15/unemployment-in-spain/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Electric cars and metal prices</title>
      <link>https://www.storieswithdata.com/post/2017/09/14/electric-cars-and-metal-prices/</link>
      <pubDate>Thu, 14 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.storieswithdata.com/post/2017/09/14/electric-cars-and-metal-prices/</guid>
      <description>In this post we will analyze the evolution of metal prices on the back of the Electric car revolution
 Intro The electric vehicles industry is creating winners and losers within the world’s biggest metals markets.
In this regard, Cobalt is essential for lithium-ion batteries powering anything from Tesla Inc.’s cars to Apple Inc.’s iPhones and iPads. [^1]
 Libraries We first install and load the libraries.
library(ggplot2) # For amazing charts library(dplyr) # Data wrangling library(hrbrthemes) # Aesthetic defaults for ggplot2 charts library(Quandl) # Get financial data library(magrittr) # Pipes %&amp;gt;% library(tidyverse) # Data wrangling packages  Importing data We start by getting the prices for Cobalt, Nickel, Aluminium and Copper.</description>
    </item>
    
    <item>
      <title>Tennis winners - scraping and plotting</title>
      <link>https://www.storieswithdata.com/post/2017/09/14/tennis-winners-scrapig-and-plotting/</link>
      <pubDate>Thu, 14 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.storieswithdata.com/post/2017/09/14/tennis-winners-scrapig-and-plotting/</guid>
      <description>Intro  In this tutorial, we will be plotting tennis players Grand Slam Tournament wins.
 The data comes from the Tennis Grand Slam Tournaments results from the ESP website. [^1]
We will scrape the data and continue with the visualization thereafter.
 Libraries Overall, we are going to use the following R packages:
library(XML) # for web scraping library(rvest) # for web scraping library(dplyr) # for dataset manipulation library(knitr) # for nice dataset printing library(hrbrthemes) # Aesthetic defaults for ggplot2 charts library(ggthemes) # Themes ggplot2 library(ggplot2) # Charts library(stringr) # Manipulating strings  Scraping We will be scraping HTML Table with the XML package.</description>
    </item>
    
    <item>
      <title>BiciMad and Heatmaps</title>
      <link>https://www.storieswithdata.com/post/2017/09/13/bicimad-and-heatmaps/</link>
      <pubDate>Wed, 13 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.storieswithdata.com/post/2017/09/13/bicimad-and-heatmaps/</guid>
      <description>In this post we will analyze Madrid’s electric bike hire service, BiciMAD (in London Santander Cycles, in Paris Vélib, etc…). We will create calendar heatmaps and a growth chart using the publicly available data.
 Libraries We first install and load the libraries.
library(ggplot2) # For amazing charts library(tidyverse) # Data wrangling library(hrbrthemes) # Aesthetic defaults for ggplot2 charts library(readr) # Reading data from csv library(formattable) # Friendy data frame can be rendered as HTML table library(lubridate) # Makes it easier to work with dates and times  Importing data The data can be obtained from datosabiertos - the free and open data-sharing portal where anyone can access data relating to the Madrid 1.</description>
    </item>
    
    <item>
      <title>Plotting long/lat in a map from a json document</title>
      <link>https://www.storieswithdata.com/post/2017/09/12/plotting-long-lat-in-a-map-from-a-json-document/</link>
      <pubDate>Tue, 12 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.storieswithdata.com/post/2017/09/12/plotting-long-lat-in-a-map-from-a-json-document/</guid>
      <description>Today we will extract information from a JSON document using the R package jsonlite and we will then plot lon/lat in a map using the R package leaflet.
The data that we will be looking at is crime in LA.
But first, we need to install and load libraries for this excercise.
library(jsonlite) library(tidyverse) library(stringr) library(tibble) library(leaflet) library(ggmap) library(readr) Now we can start grabbing the data from the json document which can be found here 1.</description>
    </item>
    
    <item>
      <title>Joyplots with ggjoy &#43; hrbrthemes</title>
      <link>https://www.storieswithdata.com/post/2017/09/11/ggjoy-hrbrthemes/</link>
      <pubDate>Mon, 11 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.storieswithdata.com/post/2017/09/11/ggjoy-hrbrthemes/</guid>
      <description>Haven’t you run into articles with some data that makes you think, how can visualize the data in a different and possibly better way?
 We will be use Joyplots for this post. Joyplots are partially overlapping line plots that create the impression of a mountain range. They can be quite useful for visualizing changes in distributions over time or space1.
The data that we will be looking at is 2016 temperatures in Lincoln, NE23.</description>
    </item>
    
    <item>
      <title>A Quick Note on Building Websites</title>
      <link>https://www.storieswithdata.com/note/2017/09/09/a-quick-note/</link>
      <pubDate>Sat, 09 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.storieswithdata.com/note/2017/09/09/a-quick-note/</guid>
      <description>Note: Thanks to those who share their ideas and help others make things happen seamlessly.
 Finally! I now have a blog. All thanks to the R package Blogdown. With Blogdown you can creat a personal website.
 After reading many articles, watching videos and multiple failed attempts, I was able to get my picked Hugo theme up and running with R Studio and Blogdown.
There are many articles on the topic but the place I strongly recommend you visit is a youtube video by John Muschelli:</description>
    </item>
    
    <item>
      <title>Pablo Ugarte</title>
      <link>https://www.storieswithdata.com/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.storieswithdata.com/about/</guid>
      <description>After a hard time getting around creating the blog, here it is!
The goal of this blog is to collect, inspect, cleanse, transform, and visualize data to discover useful information and suggest conclusions. Data that we can find anywhere these days thanks to the web. The idea is ultimatley to have fun working with such data. We will be using R to perform all the analysis - and in particular dplyr.</description>
    </item>
    
  </channel>
</rss>